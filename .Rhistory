library(rlist)
li = list('java','python')
li2 <- append(li,'r')
print(li2)
print(prems)
prems <- list()
for (i in seq(87,113, .5)){
values <- binTree(S=100,K=i,r=0.03,sigma=.18,TIME=.5,N=20)
prems <- append(prems,values$premia[1,1])
}
print(prems)
prems <- list()
for (i in seq(87,113, .5)){
values <- binTree(S=100,K=i,r=0.03,sigma=.18,TIME=.5,N=20)
prems <- append(prems,values$premia[1,1])
}
plot(seq(87,113, .5), prems)
install.packages(Thermimage)
install.packages("Thermimage")
library(Thermimage)
slopeEveryN(prems, n=1)
(slopeEveryN(prems, n=1))
prev = 0
for (i in prems){
slope <- (prev - i) / .5
print("Between :",prev, "and", i, "The slope is:", slope)
prev <- i
}
prev = 0
for (i in prems){
slope <- (prev - i) / .5
print(prev)
print(i)
print("slope:")
print(slope)
prev <- i
}
prev = 0
for (i in prems){
slope <- (i-prev) / .5
print(prev)
print(i)
print("slope:")
print(slope)
prev <- i
}
return(exp(-r*T)*avePayoff) }
binomPut <- function(K,n=4,T=1,S0=100,r=0.04,u=1.03,d=100/103) {
h <- T/n
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(Sbin-K,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*T)*avePayoff)
}
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,N=20)
binomPut <- function(K,n=4,TIME=1,S0=100,r=0.04,sigma=0.2) {
h <- TIME/n
u <- exp(h*(r-delta)+sigma*(h)^.5)
d <- exp(h*(r-delta)-sigma*(h)^.5)
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(Sbin-K,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*T)*avePayoff)
}
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,N=20)
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,n=20)
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,n=20)
binomPut <- function(K,n=4,TIME=1,S0=100,r=0.04,sigma=0.2) {
h <- TIME/n
u <- exp(h*(r)+sigma*(h)^.5)
d <- exp(h*(r)-sigma*(h)^.5)
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(Sbin-K,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*T)*avePayoff)
}
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,n=20)
price
prems <- list()
for (i in seq(87,113, .5)){
value <- binomPut(S0=100,K=i,r=0.04,sigma=.18,TIME=.5,n=20)
prems <- append(prems,value)
}
plot(seq(87,113, .5), prems)
slopes = list()
slopes = data.frame(colnames=c("1","2","slope"))
slopes = data.frame(1,2,"slopes")
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
"""print(prev)
print(i)
print()
print(slope)
prev <- i"""
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
}
print(slopes)
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
prev = i
}
print(slopes)
plot(slopes)
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
prev = i
}
print(slopes)
plot(seq(87,113, .5),slopes$slope)
plot(seq(87,113, .5),slopes$X.slopes.)
print(slopes)
plot(seq(87,113, .5),slopes$X.slopes.)
plot(seq(87,113, .5),slopes$X.slopes.[2:])
print(slopes)
plot(seq(87,113, .5),slopes$X.slopes.[2:])
print(slopes)
prems <- list()
for (i in seq(87,113, .5)){
value <- binomPut(S0=100,K=i,r=0.04,sigma=.18,TIME=.5,n=20)
prems <- append(prems,value)
}
plot(seq(87,113, .5), prems)
binTree <- function(S=49, K=50,N=10,r=0.05,delta=0,sigma=0.2, TIME=1) {
h <- TIME/N
u <- exp(h*(r-delta)+sigma*(h)^.5)
d <- exp(h*(r-delta)-sigma*(h)^.5)
disc <- exp(-r*h)
q <- (exp( (r-delta)*h)-d)/(u-d)
V <- array(0, dim=c(N+1,N+1)) # matrix for storing all the option premia at each node
Del <- B <- array(0, dim=c(N+1,N+1))
for (i in 0:(N)) { # terminal payoff
finalS <- S*(u)^i*(d)^(N-i)
V[N+1,i+1] <- max( finalS-K, 0) # Call payoff
}
V_rn=V
for (j in (N):1) { # column in the tree
for (i in 0:(j-1)) { # i counts number of up-moves in the j-th column
curS = S * u^(i) * d^(j-1-i)
B[j,i+1] = disc*( u*V[j+1,i+1] - d*V[j+1,i+2] )/(u - d)
Del[j,i+1] = exp(-delta*h)*( V[j+1,i+2] - V[j+1,i+1] )/(curS * u - curS * d)
V[j,i+1] = Del[j,i+1]*curS + B[j,i+1]
V_rn[j,i+1] <- disc*( q*V_rn[j+1,i+2] + (1-q)*V_rn[j+1,i+1] )
}
}
Delta0 <- exp(-delta*h)*(V[2,2]-V[2,1])/(S*(u-d)) # V[2,2] = C_u, V[2,1]=C_d
return (list(premia = V,Delta=Del,Bank=B) )
}
binomPut <- function(K,n=4,TIME=1,S0=100,r=0.04,sigma=0.2) {
h <- TIME/n
u <- exp(h*(r)+sigma*(h)^.5)
d <- exp(h*(r)-sigma*(h)^.5)
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(Sbin-K,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*h)*avePayoff)
}
price = binomPut(S0=100,K=87.5,r=0.04,sigma=.18,TIME=.5,n=20)
price
li = list('java','python')
li2 <- append(li,'r')
print(li2)
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
prev = i
}
print(slopes)
binTree <- function(S=49, K=50,N=10,r=0.05,delta=0,sigma=0.2, TIME=1) {
h <- TIME/N
u <- exp(h*(r-delta)+sigma*(h)^.5)
d <- exp(h*(r-delta)-sigma*(h)^.5)
disc <- exp(-r*h)
q <- (exp( (r-delta)*h)-d)/(u-d)
V <- array(0, dim=c(N+1,N+1)) # matrix for storing all the option premia at each node
Del <- B <- array(0, dim=c(N+1,N+1))
for (i in 0:(N)) { # terminal payoff
finalS <- S*(u)^i*(d)^(N-i)
V[N+1,i+1] <- max( finalS-K, 0) # Call payoff
}
V_rn=V
for (j in (N):1) { # column in the tree
for (i in 0:(j-1)) { # i counts number of up-moves in the j-th column
curS = S * u^(i) * d^(j-1-i)
B[j,i+1] = disc*( u*V[j+1,i+1] - d*V[j+1,i+2] )/(u - d)
Del[j,i+1] = exp(-delta*h)*( V[j+1,i+2] - V[j+1,i+1] )/(curS * u - curS * d)
V[j,i+1] = Del[j,i+1]*curS + B[j,i+1]
V_rn[j,i+1] <- disc*( q*V_rn[j+1,i+2] + (1-q)*V_rn[j+1,i+1] )
}
}
Delta0 <- exp(-delta*h)*(V[2,2]-V[2,1])/(S*(u-d)) # V[2,2] = C_u, V[2,1]=C_d
return (list(premia = V,Delta=Del,Bank=B) )
}
binomPut <- function(K,n=4,TIME=1,S0=100,r=0.04,sigma=0.2) {
h <- TIME/n
u <- exp(h*(r)+sigma*(h)^.5)
d <- exp(h*(r)-sigma*(h)^.5)
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(K-Sbin,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*h)*avePayoff)
}
prems <- list()
for (i in seq(87,113, .5)){
value <- binomPut(S0=100,K=i,r=0.04,sigma=.18,TIME=.5,n=20)
prems <- append(prems,value)
}
plot(seq(87,113, .5), prems)
slopes = data.frame(1,2,"slopes")
prev = 0
for (i in prems){
slope <- (i-prev) / .5
slopes <- rbind(slopes, list(prev, i, slope))
prev = i
}
print(slopes)
binomCall <- function(K,n=4,TIME=1,S0=100,r=0.04,sigma=0.2) {
h <- TIME/n
u <- exp(h*(r)+sigma*(h)^.5)
d <- exp(h*(r)-sigma*(h)^.5)
q <- (exp(r*h)-d)/(u-d)
Sbin <- S0*d^n*(u/d)^(0:n)
qbin <- dbinom(0:n,n,q)
Payoff <- pmax(Sbin-K,0)
avePayoff <- sum(qbin*Payoff)
return(exp(-r*h)*avePayoff)
}
binomCall(K=50,n=4,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=4,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=8,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=15,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=30,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=100,TIME=1,S0=50,r=0.05,sigma=0.2)
binomCall(K=50,n=150,TIME=1,S0=50,r=0.05,sigma=0.2)
setwd("~/GitHub/ML-final-project")
library(tidyverse)
library(ISLR)
library(dplyr)
library(tidyr)
library(caret)
library(keras)
data <- read.csv("data/final_dataset_cleaned.csv")
# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")
# convert USREC columns to "Yes" or "No" factors
data <- data %>%
mutate_at(vars(USREC1, USREC3, USREC6, USREC12), ~factor(ifelse(. == 1, "Yes", "No")))
set.seed(333)
#Standardizing Data Values
standard.data <- data %>% mutate_at(c(2:11), ~(scale(.) %>% as.vector))
#function to copy data to create larger dataset for better performance
copy_data <- function(array, n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
#Setting up test and training datasets
set.seed(123)
test = sample(1:nrow(standard.data), 102)
test.recc = standard.data[test,]
train.recc = standard.data[-test,]
train.x <- train.recc[,c(2:11)]
test.x <- test.recc[,c(2:11)]
train.y.1m <- train.recc[,13]
train.y.3m <- train.recc[,14]
train.y.6m <- train.recc[,15]
train.y.12m <- train.recc[,16]
test.y.1m <- test.recc[,13]
test.y.3m <- test.recc[,14]
test.y.6m <- test.recc[,15]
test.y.12m <- test.recc[,16]
#To avoid stacking the model everytime the program is run
rm(model)
layer_array <- c(7,6)
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
use_condaenv("r-tensorflow")
library(reticulate)
conda_version()
reticulate:::find_conda()
reticulate:::find_conda()
reticulate:::find_conda()
for(i in 1:n){array <- rbind(array,array)}
library(tidyverse)
library(ISLR)
library(dplyr)
library(tidyr)
library(caret)
library(keras)
data <- read.csv("data/final_dataset_cleaned.csv")
# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")
# convert USREC columns to "Yes" or "No" factors
data <- data %>%
mutate_at(vars(USREC1, USREC3, USREC6, USREC12), ~factor(ifelse(. == 1, "Yes", "No")))
set.seed(333)
#Standardizing Data Values
standard.data <- data %>% mutate_at(c(2:11), ~(scale(.) %>% as.vector))
#function to copy data to create larger dataset for better performance
copy_data <- function(array, n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
#Setting up test and training datasets
set.seed(123)
test = sample(1:nrow(standard.data), 102)
test.recc = standard.data[test,]
train.recc = standard.data[-test,]
train.x <- train.recc[,c(2:11)]
test.x <- test.recc[,c(2:11)]
train.y.1m <- train.recc[,13]
train.y.3m <- train.recc[,14]
train.y.6m <- train.recc[,15]
train.y.12m <- train.recc[,16]
test.y.1m <- test.recc[,13]
test.y.3m <- test.recc[,14]
test.y.6m <- test.recc[,15]
test.y.12m <- test.recc[,16]
#To avoid stacking the model everytime the program is run
rm(model)
layer_array <- c(7,6)
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
use_condaenv("r-tensorflow")
install.packages("tensorflow")
install.packages("tensorflow")
library(tensorflow)
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
array <- c(7,6)
model <- keras_model_sequential() %>%
layer_dense(units = array[1], activation = "sigmoid", input_shape = c(10)) %>%
layer_dense(units = array[2], activation = "sigmoid") %>%
layer_dense(units = 2, activation = "softmax")
install_keras(tensorflow="nightly")
install_keras(")
install_keras()
library(caret)
library(keras)
#Setting up test and training datasets
set.seed(123)
test = sample(1:nrow(standard.data), 102)
test.recc = standard.data[test,]
train.recc = standard.data[-test,]
train.x <- train.recc[,c(2:11)]
test.x <- test.recc[,c(2:11)]
train.y.1m <- train.recc[,13]
train.y.1m
train.y.1m
install_keras()
install.packages("neuralnet")
nn <- neuralnet(USREC1~.-DATE-USREC-USREC3-USREC6-USREC12,
data = standard.data,
hidden = c(7,6),
linear.output = TRUE,
lifesign = "full",
rep = 1)
library(neuralnet)
nn <- neuralnet(USREC1~.-DATE-USREC-USREC3-USREC6-USREC12,
data = standard.data,
hidden = c(7,6),
linear.output = TRUE,
lifesign = "full",
rep = 1)
plot(nn)
plot(nn,
col.hidden="darkgreen",
col.hidden.synapse="orange",
show.weights=F,
information=F,
fill= "lightblue")
model <- keras_model_sequential()
library(keras)
library(tidyverse)
library(ISLR)
library(dplyr)
library(tidyr)
library(caret)
library(keras)
library(tensorflow)
library(neuralnet)
data <- read.csv("data/final_dataset_cleaned.csv")
# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")
# convert USREC columns to "Yes" or "No" factors
data <- data %>%
mutate_at(vars(USREC1, USREC3, USREC6, USREC12), ~factor(ifelse(. == 1, "Yes", "No")))
set.seed(333)
#Standardizing Data Values
standard.data <- data %>% mutate_at(c(2:11), ~(scale(.) %>% as.vector))
#function to copy data to create larger dataset for better performance
copy_data <- function(array, n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
#Setting up test and training datasets
set.seed(123)
test = sample(1:nrow(standard.data), 102)
test.recc = standard.data[test,]
train.recc = standard.data[-test,]
train.x <- train.recc[,c(2:11)]
test.x <- test.recc[,c(2:11)]
train.y.1m <- train.recc[,13]
train.y.3m <- train.recc[,14]
train.y.6m <- train.recc[,15]
train.y.12m <- train.recc[,16]
test.y.1m <- test.recc[,13]
test.y.3m <- test.recc[,14]
test.y.6m <- test.recc[,15]
test.y.12m <- test.recc[,16]
#To avoid stacking the model everytime the program is run
rm(model)
array <- c(7,6)
model <- keras_model_sequential()
library(reticulate)
virtualenv_create("r-reticulate", python = install_python())
virtualenv_create("r-reticulate", python = install_python())
library(tidyverse)
library(ISLR)
library(dplyr)
library(tidyr)
library(caret)
library(keras)
library(tensorflow)
library(neuralnet)
data <- read.csv("data/final_dataset_cleaned.csv")
# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")
# convert USREC columns to "Yes" or "No" factors
data <- data %>%
mutate_at(vars(USREC1, USREC3, USREC6, USREC12), ~factor(ifelse(. == 1, "Yes", "No")))
set.seed(333)
#Standardizing Data Values
standard.data <- data %>% mutate_at(c(2:11), ~(scale(.) %>% as.vector))
#function to copy data to create larger dataset for better performance
copy_data <- function(array, n)
{
for(i in 1:n){array <- rbind(array,array)}
return(array)
}
#Setting up test and training datasets
set.seed(123)
test = sample(1:nrow(standard.data), 102)
test.recc = standard.data[test,]
train.recc = standard.data[-test,]
train.x <- train.recc[,c(2:11)]
test.x <- test.recc[,c(2:11)]
train.y.1m <- train.recc[,13]
train.y.3m <- train.recc[,14]
train.y.6m <- train.recc[,15]
train.y.12m <- train.recc[,16]
test.y.1m <- test.recc[,13]
test.y.3m <- test.recc[,14]
test.y.6m <- test.recc[,15]
test.y.12m <- test.recc[,16]
#To avoid stacking the model everytime the program is run
rm(model)
array <- c(7,6)
model <- keras_model_sequential() %>%
layer_dense(units = layer_array[1], activation="sigmoid", input_shape = c(10)) %>%
layer_dense(units = layer_array[2], activation="sigmoid") %>%
layer_dense(units = 2, activation="softmax")
summary(model)
model <- keras_model_sequential()
