---
title: "PSTAT131: Machine Learning Final Project"
subtitle: ""
author: "Rohan Anand, Coby Wilcox"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
library(ggplot2)
library(corrplot)
library(reshape2)
library(ggcorrplot)
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(dplyr)

setwd("/Users/rohananand/Documents/GitHub/ML-final-project")
data <- read.csv("data/final_dataset_cleaned.csv")

# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")
```

## Objective

This report investigates the predictive power of various economic indicators, including S&P500 data, Treasury Bond yields, unemployment rate, real personal income, industrial production, real GDP, and more, to develop a machine learning model for forecasting economic recessions **within the next 12 months**. The employed machine learning methods encompass **logistic regression**, **ridge and lasso regression**, **random forests** with and without boosting for ensemble-based learning, and **neural networks**, which capture complex relationships within the data. Inspired by contemporary economic trends, this analysis aims to employ machine learning techniques to enhance recession prediction accuracy and serve as a starting point to contribute valuable insights for informed decision-making.

## Methodology

```{mermaid}
flowchart LR
  A[Preprocessed Data] --> B(Exploratory Data Analysis)
  B --> C[Logistic Regression]
  B --> D[Ridge and Lasso Regression]
  B --> E[Random Forests]
  B --> F[Neural Networks]
  C --> G[Evaluate Performance]
  D --> G
  E --> G
  F --> G
```

Diagram of the workflow for the methodologies that we will explore in our analysis

## Dataset

This dataset encompasses a diverse array of critical economic indicators, providing a comprehensive view of the U.S. economic landscape. The dataset includes 502 observations for the following from 1982 to 2023, updated in monthly intervals:

+--------------------------------------------+----------------------------------+
| -   10-year & 3-month treasury bond yields | -   National unemployment rate   |
+--------------------------------------------+----------------------------------+
| -   Real personal income                   | -   Number of U.S. workers       |
+--------------------------------------------+----------------------------------+
| -   Industrial Production                  | -   Real retail sales            |
+--------------------------------------------+----------------------------------+
| -   AAA/BAA yield curve of corporate bonds | -   Heavy Truck Sales            |
+--------------------------------------------+----------------------------------+
| -   Consumer Price Index (CPI)             | -   S&P 500 Returns              |
+--------------------------------------------+----------------------------------+

For the purposes of our analysis, we must explicity define what constitutes a recession. The U.S. government typically defines a recession as a sustained period of economic decline marked by a contraction in real GDP for two consecutive quarters (\~6 months). This widely accepted definition captures the essence of a recession, reflecting reduced economic activity, rising unemployment, and potential impacts on various sectors, all of which are crucial factors monitored within this dataset to better understand and predict economic cycles. Thus, we will stick with the government definition of a recession.

## Exploratory Data Analysis

In the following section, we conduct a thorough exploratory data analysis (EDA) of key economic indicators, examining their relationships and seeking insights into trends and patterns. The objective is to gain a comprehensive understanding of the purpose of these indicators, how they have evolved over time, and to uncover potential correlations and influential factors.

**10-Year Treasury Maturity Minus 3-Month Treasury Constant Maturity (T10Y3M):** The difference between the 10 year treasury rate and the 3 month treasury rate.  Historically shown to be one of the most accurate indicators of an incoming recession. The flattening and especially the inverting (the 3 month rate being higher than the 10 year rate) is the go-to precursor to a recession. The New York Fed uses the rate in a model to predict recessions 2 to 6 quarters ahead.

```{r}
# Time vs T10Y3M
ggplot(data, aes(x = DATE, y = T10Y3M, color = T10Y3M)) +
  geom_line() +
  labs(title = "Time vs T10Y3M",
       x = "Time",
       y = "T10Y3M") +
  theme_minimal() 

```

**National Unemployment Rate (UNRATE):** The unemployment rate represents the number of unemployed as a percentage of the labor force. Labor force data are restricted to people 16 years of age and older, who currently reside in 1 of the 50 states or the District of Columbia, who do not reside in institutions (e.g., penal and mental facilities, homes for the aged), and who are not on active duty in the Armed Forces. Has been shown in other studies to be a reliable predictor of recessions.

```{r}
# Time vs UNRATE
ggplot(data, aes(x = DATE, y = UNRATE, color = UNRATE)) +
  geom_line() +
  labs(title = "Time vs UNRATE",
       x = "Time",
       y = "UNRATE") +
  theme_minimal() 
```

**Real Personal Income Excluding Transfers (RPIT):** Real Personal Income Excluding Transfers is an economic indicator that measures the income received by individuals and households from all sources, excluding government transfers. Government transfers include social welfare payments such as unemployment benefits, Social Security, and other forms of assistance. The "real" aspect of Real Personal Income refers to the fact that the data is adjusted for inflation, providing a more accurate measure of changes in purchasing power over time. Adjusting for inflation allows economists and policymakers to compare income levels across different time periods in terms of constant purchasing power.

```{r}
# Time vs RPIT
ggplot(data, aes(x = DATE, y = RPIT, color = RPIT)) +
  geom_line() +
  labs(title = "Time vs RPIT",
       x = "Time",
       y = "RPIT") +
  theme_minimal() 
```

**All Employees: Total Nonfarm (PAYEMS):** commonly known as Total Nonfarm Payroll, is a measure of the number of U.S. workers in the economy that excludes proprietors, private household employees, unpaid volunteers, farm employees, and the unincorporated self-employed. This measure accounts for approximately 80 percent of the workers who contribute to Gross Domestic Product (GDP). This measure provides useful insights into the current economic situation because it can represent the number of jobs added or lost in an economy. Generally, the U.S. labor force and levels of employment and unemployment are subject to fluctuations due to seasonal changes in weather, major holidays, and the opening and closing of schools. The Bureau of Labor Statistics (BLS) adjusts the data to offset the seasonal effects to show non-seasonal changes.

```{r}
# Time vs PAYEMS
ggplot(data, aes(x = DATE, y = PAYEMS, color = PAYEMS)) +
  geom_line() +
  labs(title = "Time vs PAYEMS",
       x = "Time",
       y = "PAYEMS") +
  theme_minimal() 
```

**Industrial Production (INDPRO):** Industrial production measures the output of goods produced or mined in the U.S. While the manufacturing sector only makes up less than 20% of the economy, this indicator is closely watched as it is highly sensitive and quick to react to changes in the business cycle. The Federal Reserve's monthly index of industrial production and the related capacity indexes and capacity utilization rates cover manufacturing, mining, and electric and gas utilities. The industrial sector, together with construction, accounts for the bulk of the variation in national output over the course of the business cycle. The industrial detail provided by these measures helps illuminate structural developments in the economy.

```{r}
# Time vs INDPRO
ggplot(data, aes(x = DATE, y = INDPRO, color = INDPRO)) +
  geom_line() +
  labs(title = "Time vs INDPRO",
       x = "Time",
       y = "INDPRO") +
  theme_minimal() 
```

**Real Retail Sales (RRS):** This factor is found by splicing two different statistics recorded by the US Census Bureau. Retail Sales (RETAIL), a combined value of sales and end of month inventory by stores across the US, was discontinued in 2001, and Advanced Retail Sales: Retail Trade and Food Services (RSAFS) started in 1992 and continues to this day. The difference between the two statistics seems quite marginal, with RSAFS having more advanced statistical procedures being done to it. To account for seasonal changes in consumer spending, the splice should also be deflated by the seasonally adjusted Consumer Price Index.

```{r}
# Time vs RSAFS
ggplot(data, aes(x = DATE, y = RSAFS, color = RSAFS)) +
  geom_line() +
  labs(title = "Time vs RSAFS",
       x = "Time",
       y = "RSAFS") +
  theme_minimal() 
```

**Corporate Bond AAA/BAA Yield Curve (CORP):** To give accurate estimates at the creditworthiness of corporate bonds companies like Moody, Standard and Poor\'s, and Fitch have developed rating systems. The systems can differ in small ways but generally the highest rated companies have AAA, then the next highest have AA, then A and BAA or BBB and so on. This dataset will specifically use Moody\'s ratings and display the difference between the AAA rated bonds and BAA similar to the system in T10Y3M. Though the relationship found between CORP and recessions is quite different to T10Y3M, with AAA ratings being too high compared to BAA ratings being associated with a likely recession as opposed to them being rated too low. 

```{r}
# Time vs CORP
ggplot(data, aes(x = DATE, y = CORP, color = CORP)) +
  geom_line() +
  labs(title = "Time vs CORP",
       x = "Time",
       y = "CORP") +
  theme_minimal() 
```

**Heavy Truck Sales (HTS):** Heavy truck sales have been known to be a surprisingly good predictor of US recessions. Since 1973 there have been 7 recessions and according to Joseph Calhoun of Seeking Alpha, heavy trucks have been a predictor for every single one of them. The logic is that during an economic boom people will want to buy heavy trucks to transport goods to make more money, but once money gets tight commerce slows, and heavy truck sales will begin to fall.

```{r}
# Time vs HTS
ggplot(data, aes(x = DATE, y = HTS, color = HTS)) +
  geom_line() +
  labs(title = "Time vs HTS",
       x = "Time",
       y = "HTS") +
  theme_minimal() 
```

**Consumer Price Index: All Items: Total For United States (CPIAI) -** A standardized growth rate of consumer prices for all items bought within the United States. This dataset is not seasonally adjusted. This data has also not historically shown to be a predictor of recessions. The purpose of this predictor is mainly as a dummy variable.

```{r}
# Time vs CPIAI
ggplot(data, aes(x = DATE, y = CPIAI, color = CPIAI)) +
  geom_line() +
  labs(title = "Time vs CPIAI",
       x = "Time",
       y = "CPIAI") +
  theme_minimal() 
```

**Correlation Plot:**

```{r}
dateless_data <- data[, !names(data) %in% c("DATE", "USREC", "USREC1", "USREC3", "USREC6", "USREC12")]
correlation_matrix <- cor(dateless_data)
ggcorrplot(correlation_matrix, type = "lower", outline.col = "white")
```

As you can see, CPIAI has the most positive correlation with the other factors. It seems that the SP500R predictor has very little to no correlation with any of the other factors.

## Results 

**Logistic Regression:** We implement a logistic regression as a benchmark method. In general, a logistic regression may struggle in predicting recessions due to its linear assumptions, limited ability to capture the complex and nonlinear relationships inherent in economic data, challenges with feature independence and changing feature importance over time, sensitivity to non-stationarity in economic variables, potential bias in handling imbalanced data where recessions are infrequent, and the difficulty in satisfying model assumptions. Additionally, logistic regression may not account for unobserved external factors influencing recessions, and its performance can be hindered by noisy or error-prone economic data. To improve recession prediction, we consider more sophisticated modeling approaches. The summary statistics for the logistic regression model implemented are below. Note that the logistic regression is able to pick up the majority of the important predictors.

```{r}
library(tidyverse)
library(ISLR)
library(glmnet)
library(tree)
library(maptree)
library(randomForest)
library(gbm)
library(ROCR)
library(dplyr)

setwd("/Users/rohananand/Documents/GitHub/ML-final-project")
data <- read.csv("data/final_dataset_cleaned.csv")

# adjust DATE formatting
data$DATE <- as.POSIXct(data$DATE, format = "%m/%d/%Y")

# convert USREC columns to "Yes" or "No" factors
data <- data %>%
  mutate_at(vars(USREC1, USREC3, USREC6, USREC12), ~factor(ifelse(. == 1, "Yes", "No")))

#implement a logistic regression as a benchmark method 
logistic_data_12 <- glm(USREC12~ .-DATE -USREC -USREC1 -USREC3 -USREC6,data=data,family = "binomial")
summary(logistic_data_12)
```

**Regularization Methods**: Lasso and Ridge regression are both a form of \"regularization\" machine learning methods. Regularization refers to their method of pushing ordinary least squares coefficients towards zero to achieve typically more interpretable and accurate results. To achieve this regularization the methods attach a shrinkage penalty to the least squares estimate consisting of tuning parameter ג, and either a l2 norm squared or l1 norm. 

1.  **Ridge Regression**: The performance of the ridge model was unimpressive; while the accuracy ratings were all quite high around the 95% mark, the true marker of a good model for our problem must include good sensitivity. None of the ridge models beat a 0.3 sensitivity rating even in the training sets. The poor performance of the ridge model I think may be attributed to the quality of some of the predictors being mixed. In other papers on forecasting economic recessions, it seems a few predictors tend to come out on top very consistently like the T10Y3M and HTS, while others like SP500R and RGDP can be inconsistent. So Ridge's style of utilizing all of the predictors in a somewhat balanced fashion to create coefficients may not be well suited to this dataset.\

```{r}
setwd("/Users/rohananand/Documents/GitHub/ML-final-project")
data.raw <- read.csv("data/final_dataset_cleaned.csv")

set.seed(333)
cols = c("USREC","USREC1","USREC3","USREC6","USREC12")
data.raw[cols] <- lapply(data.raw[cols], factor) # converts the last four columns to factors

# creating training and test data

test = sample(1:nrow(data.raw), 102)
test.recc = data.raw[test,]
train.recc = data.raw[-test,]
train.recc[cols] <- lapply(train.recc[cols], factor)

x.train = train.recc[2:11]

x.test = test.recc[2:11]
y.test.1m = test.recc["USREC1"]
y.test.3m = test.recc["USREC3"]
y.test.6m = test.recc["USREC6"]
y.test.12m = test.recc["USREC12"]

standard.data <- data.raw %>% mutate_at(c(4,5,6,7,10), ~(scale(.) %>% as.vector))
matrix.data.12 <- model.matrix(USREC12~.-DATE-USREC-USREC3-USREC6-USREC1, standard.data)

test.recc.standard.12 = matrix.data.12[test,]
train.recc.standard.12 = matrix.data.12[-test,]

y.train.12m = train.recc$USREC12

#x.test.standard = test.recc.standard[2:11]

y.test.12m = test.recc$USREC12

lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

ridge_model_12 <- glmnet(train.recc.standard.12, y.train.12m, alpha=0, lamda=lambda.list.ridge, family = "binomial")

cv.out.ridge.12 <- cv.glmnet(train.recc.standard.12, y.train.12m, alpha=0,folds=5, family="binomial")
plot.new()
plot(cv.out.ridge.12)
```

2.  **Lasso Regression**: Lasso\'s performance seemed to be a massive improvement compared to ridge. Similar testing to the ridge model yielded multiple models with over 0.60 test sensitivity ratings for models predicting recessions within the next month or next year. A 60% sensitivity may not sound amazing, but with such heavy impact events like economic recessions, a 60% certainty of one occurring within a month could be of incredible use to some individuals or companies. Another result of the LASSO models worth looking into is their subset selection. By the nature of how LASSO coefficients are calculated some coefficients will become zero and essentially be subsetted out of the model. LASSO subsetting is somewhat close to \"best subset selection\" so seeing what is and is not subsetted out of the model can be very revealing to predictor importance. 

```{r}
lasso_model_12 <- glmnet(train.recc.standard.12, y.train.12m, alpha=1,lamda=lambda.list.ridge, family = "binomial")

cv.out.lasso.12 = cv.glmnet(train.recc.standard.12, y.train.12m, alpha = 1, family = "binomial")

plot(cv.out.lasso.12)
abline(v = log(cv.out.lasso.12$lambda.min), col="red", lwd=3, lty=2)
```

Using the two best performing models 1 month and 12 month, we then created full models utilizing the entire dataset and then looked at the resulting coefficients estimates. Some of most interesting observations are that:

1.  HTS was in both cases the highest weighted coefficient in the model suggesting a very solid correlation between Heavy Truck Sales and recessions.
2.  T10Y3M while never being subsetted out fully was in both cases given one of the lowest coefficient weights somewhat contradicting previous results in recession forecasting.

**Decision Trees:** We start off by constructing the most basic building block of a random forest: the decision tree. The primary advantage of decision trees is its high interpretability. The decision tree systematically partitions the dataset of economic indicators based on feature values to create a predictive model, recursively splitting the data into subsets by identifying the most informative features at each node, leading to a tree-like structure that represents decision rules for classifying or predicting outcomes.

```{r}
set.seed(123)

tree.data_12 = tree(USREC12 ~.-DATE -USREC -USREC1 -USREC3 -USREC6, data = data)
cv.data_12 <- cv.tree(tree.data_12, FUN=prune.misclass, K=5)
best_size <- min(cv.data_12$size[cv.data_12$dev == min(cv.data_12$dev)])
best_tree12.cv = prune.misclass (tree.data_12, best=best_size)

plot(best_tree12.cv)
text(best_tree12.cv, pretty=0, col = "blue", cex = .5)
title("Best Tree (of Size 11) for USREC12")
```

As you can see above, the tree data structure allows us to easily determine classification of whether a recession is imminent based on the criteria of the predictors. To find the tree with the optimal size for our dataset, we used cross-validation and found the tree with the best size is 11.

**Random Forests**: In our analysis, we constructed two random forest models employing distinct ensemble techniques: bagging and boosting.

1.  **Bagging Model:** Bagging, short for Bootstrap Aggregating, involves creating multiple subsets of the original dataset through bootstrap sampling and training individual decision trees on each subset. The final prediction is determined by averaging or voting across all trees. Our bagging random forest model demonstrated a False Positive Rate (FPR) of 0.007246 and a True Positive Rate (TPR) of 0.5385, showcasing its balanced performance in classification tasks. Additionally, the test error rate for the bagging model was 1.325%, indicating an overall accuracy of 98.675%. Although our test error rate was very low, we are mostly interested in the TPR (since we want to classify when a recession occurred). Using this metric, our model performed slightly better than random chance. The summary of all the predictors is provided below.

```{r}
#USREC12 Random Forest Model
# Create training and test datasets
index <- sample(1:nrow(data), 0.7 * nrow(data))
data_12.train <- data[index, ]
data_12.test <- data[-index, ]

# tuning hyper-parameters --> mtry: 
# the number of variables to randomly sample as candidates of a split 
rf.data12 = randomForest(USREC12 ~ .-DATE -USREC -USREC1 -USREC3 -USREC6, data=data_12.train,
                           mtry=3, importance=TRUE)

importance_scores <- importance(rf.data12)
ordered_variables <- importance_scores[order(importance_scores[, 1], decreasing = TRUE), , drop = FALSE]
print(ordered_variables)

yhat.rf = predict(rf.data12, newdata = data_12.test)
test.rf.err = mean(yhat.rf != data_12.test$USREC12)

err.pt.rf <- table(yhat.rf, data_12.test$USREC12)

#TPR = TP/TP+FN (sensitivity)
rf.tpr <- err.pt.rf[2,2]/(err.pt.rf[2,2] + err.pt.rf[1,2])

#TNR = TN/TN + FP
rf.tnr <- err.pt.rf[1,1]/(err.pt.rf[1,1] + err.pt.rf[2,1])

#FPR = FP/FP + TN
rf.fpr <- err.pt.rf[2,1] /(err.pt.rf[2,1] + err.pt.rf[1,1])
```

2.  **Boosting Model**: In contrast, boosting focuses on sequentially building decision trees, with each subsequent tree giving more weight to misclassified observations from the previous trees. This iterative process aims to improve model accuracy by emphasizing challenging instances. Our boosting random forest model displayed a True Positive Rate (TPR) of 0.6185 and a False Positive Rate 0.0122. The test error rate for the boosting model was notably lower at 0.02649, reflecting a a worse overall accuracy at 97.351%. Again, we are primarily concerned with TPR and our boosting model performed slightly better than the bagging model using that metric. Below is the importance scores of each predictor using our model.

```{r}
#Boosted tree
boosted.tree.data12 <- gbm(USREC12~.-DATE -USREC -USREC1 -USREC3 -USREC6,
                         distribution = "gaussian", data = data_12.train,
                         n.trees = 1000, shrinkage = 0.01)
summary(boosted.tree.data12)


yhat.boost = predict(boosted.tree.data12, newdata = data_12.test,
                     n.trees=1000, type = "response")

yhat.boost = ifelse(yhat.boost > 0.5, 0, 1)
test.boost.err = 1-mean(yhat.boost != ifelse(data_12.test$USREC12=="Yes",0,1))
```

**Neural Networks**: Neural Networks are the current forefront of the machine learning world, their prediction accuracy can even outperform human judgment if given enough data. \"Enough\" unfortunately typically qualifies as around 10,000,000 observations of labeled data, which is much, much larger than our relatively small dataset of 502 observations. Neural networks are very customizable though, they are built on the skeleton of linear regression model, and there is an incredible amount of freedom to play with the many parameters of neural networks, such as hidden layer number, neuron number, learning rate, activation functions, epochs, batch size, etc. 

The basic structure of a neural network as stated previously follows a skeleton of a linear regression model with the large addition of hidden layers. The hidden layer can best be described as a function of a linear combination of predictors, which ultimately are how the model is \"trained\" to perform its prediction. The functions of hidden layers can vary greatly from layer to layer and each function will typically serve to capture some bit of relevant information. 

Our implementation of neural networks utilized tensorflow and keras within a Python script. The initial implementation of the neural network yielded a model that would typically converge to a 0.9143 accuracy rated model with around 0.3 loss for the 12 month model. The 1, 3, and 6 month model would create models of similar performance with .894, .898, and .904 accuracies respectively. 

Because the 12 month model seemed to perform the best the rest of the analysis will be done solely using the 12 month model. This was however using the entire dataset to train the model with zero validation or testing done. Knowing the importance of having as much data as possible for neural networks, we decided to avoid splitting the data further in training, validation and testing sets. Instead we decided trying to estimate the accuracy of the model using cross validation would be a more apt approach. To account for the unbalanced nature of the dataset we opted to use stratified cross validation. \
The performance of the model was significantly less impressive when looking at its cross validated performance. Using the metric of ROC AUC the model on average scored around a 0.45 score. 

**Parameter Choice and Reasoning**:

To choose most of the parameters it was really just a task of trial and error. We ended up using 3 hidden layers with 6, 6, and 3 neurons respectively. We noticed significant performance decreases when increasing or decreasing these numbers at large extents, but performance would stay consistent within this range. 

**Conclusion**: Overall the neural network did not quite live up to its legendary level of performance, but most likely for very good reason. Our data size was just too small, this causes issues for any machine learning algorithms, but neural networks can be especially dependent on having ample data. So the performance ended up being around a 0.45 ROC AUC score for the highest performing model.

**Overall Project Takeaway**: After considering the generalization performance of all the models implemented, our boosting Random Forest Model performed the best via TPR and accuracy.

## Further Considerations

Time series analysis is indispensable for making informed investment decisions and managing risk. It serves as a critical tool for identifying trends and patterns within historical stock price data, enabling investors to anticipate market movements and formulate effective trading strategies. By detecting and quantifying seasonal variations and cyclical patterns in stock prices, time series analysis aids in predicting potential fluctuations and understanding market behavior.

The forecasting capabilities of time series models are particularly crucial in financial markets, where accurate predictions of future trends are essential for maximizing returns and minimizing risks. Investors and financial analysts leverage time series analysis to forecast stock prices, assess market volatility, and make data-driven decisions regarding portfolio management.

Our analysis does not consider time series analysis due to complexity and the topics being out of the scope of the class. This is probably one of the biggest faults with our analysis and it explains our lackluster performance results with our models.
